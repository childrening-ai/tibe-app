import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

# --- 1. è¨­å®šå€ (å·²æ›´æ–°ç‚º 2026 æ—¥æœŸ) ---
BASE_URL = "https://www.tibe.org.tw/tw/calendar"

# è«‹ç¢ºèªé€™äº› ID (69, 70...) å°æ‡‰åˆ°çš„ç¶²é çœŸçš„æ˜¯ 2026 çš„æ´»å‹•
# å¦‚æœå®˜ç¶²é‚„æ²’æ›´æ–° 2026ï¼ŒæŠ“åˆ°çš„å¯èƒ½æœƒæ˜¯èˆŠè³‡æ–™
DATE_MAP = {
    "2026-02-03 (äºŒ)": "69",
    "2026-02-04 (ä¸‰)": "70",
    "2026-02-05 (å››)": "71",
    "2026-02-06 (äº”)": "72",
    "2026-02-07 (å…­)": "73",
    "2026-02-08 (æ—¥)": "74",
}

# --- 2. å¼·åŠ›æ¸…æ´—å‡½å¼ (è§£æ±ºè³‡æ–™éŒ¯ç½®é—œéµ) ---
def clean_text(text):
    if not text:
        return ""
    # 1. å°‡ HTML çš„ <br> æ›æˆç©ºç™½
    # 2. å»é™¤å‰å¾Œç©ºç™½
    text = text.strip()
    # 3. å°‡å…§éƒ¨çš„æ›è¡Œç¬¦è™Ÿ (\n, \r) æ›¿æ›æˆç©ºç™½ï¼Œé¿å… CSV æ–·è¡Œ
    text = re.sub(r'[\r\n]+', ' ', text)
    # 4. å°‡é€—è™Ÿ (,) æ›¿æ›æˆå…¨å½¢é€—è™Ÿ (ï¼Œ)ï¼Œé¿å… CSV æ¬„ä½ä½ç§»
    text = text.replace(',', 'ï¼Œ')
    # 5. ç§»é™¤å¤šé¤˜çš„é€£çºŒç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    return text

def scrape_single_page(url):
    """æŠ“å–å–®ä¸€é é¢"""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return [], False
            
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.find_all(class_="calendar-item")
        
        if not items:
            return [], False

        page_events = []
        for item in items:
            title_el = item.find(class_="header-text")
            if not title_el: continue
            
            # åˆå§‹åŒ–æ¬„ä½
            event_data = {
                "æ—¥æœŸ": "", # ç¨å¾Œå¡«å…¥
                "æ™‚é–“": "", 
                "æ´»å‹•åç¨±": clean_text(title_el.text),
                "åœ°é»": "", 
                "ä¸»è¬›äºº": "", 
                "ä¸»æŒäºº": "", 
                "é¡å‹": "è¬›åº§",
                "å‚™è¨»": "",
                "è©³ç´°å…§å®¹": ""
            }
            
            # è§£æ Info å€å¡Š (æ™‚é–“/åœ°é»/ä¸»è¬›)
            for label in item.find_all(class_="info-name"):
                val = label.find_next_sibling(class_="info-text")
                if not val: val = label.find_previous_sibling(class_="info-text")
                
                if val:
                    # ğŸ”¥ é€™è£¡æ¯ä¸€é …éƒ½ç¶“é clean_text æ¸…æ´—
                    txt = clean_text(val.text)
                    if "æ™‚é–“" in label.text: event_data["æ™‚é–“"] = txt
                    elif "åœ°é»" in label.text: event_data["åœ°é»"] = txt
                    elif "ä¸»è¬›" in label.text: event_data["ä¸»è¬›äºº"] = txt
                    elif "ä¸»æŒ" in label.text: event_data["ä¸»æŒäºº"] = txt
            
            # è§£æè©³ç´°å…§å®¹
            desc = item.find(class_="web-editor")
            if desc:
                # ğŸ”¥ è©³ç´°å…§å®¹æœ€å®¹æ˜“å‡ºäº‹ï¼Œä¸€å®šè¦æ¸…æ´—æ›è¡Œ
                full_text = clean_text(desc.text)
                event_data["è©³ç´°å…§å®¹"] = full_text
                # å‚™è¨»åªå–å‰ 30 å­—
                event_data["å‚™è¨»"] = full_text[:30] + "..." if len(full_text) > 30 else full_text
            
            # ç°¡å–®é¡å‹åˆ¤æ–·
            name_chk = event_data["æ´»å‹•åç¨±"]
            loc_chk = event_data["åœ°é»"]
            if "ç°½æ›¸" in name_chk or "ç°½å" in name_chk: event_data["é¡å‹"] = "ç°½æ›¸æœƒ"
            elif "ç›´æ’­" in loc_chk: event_data["é¡å‹"] = "ç›´æ’­æ´»å‹•"
            elif "æ²™é¾" in loc_chk: event_data["é¡å‹"] = "æ²™é¾è¬›åº§"
            elif "DIY" in name_chk or "æ‰‹ä½œ" in name_chk: event_data["é¡å‹"] = "æ‰‹ä½œæ´»å‹•"

            page_events.append(event_data)
            
        return page_events, True
        
    except Exception as e:
        print(f"âš ï¸ çˆ¬èŸ²éŒ¯èª¤: {e}")
        return [], False

def main():
    print("ğŸš€ é–‹å§‹æŠ“å–è³‡æ–™ (2026 æ—¥æœŸä¿®æ­£ç‰ˆ)...")
    all_data = []
    
    for date_str, date_id in DATE_MAP.items():
        # åªå–æ—¥æœŸéƒ¨åˆ†ï¼Œä¾‹å¦‚ "2026-02-03" (å»é™¤æ˜ŸæœŸå¹¾ï¼Œç‚ºäº† CSV ä¹¾æ·¨)
        clean_date_only = date_str.split(" ")[0]
        
        print(f"\nğŸ“… æ­£åœ¨è™•ç†: {date_str} (ID: {date_id})")
        page = 1
        
        while True:
            url = f"{BASE_URL}/{date_id}?page={page}"
            # print(f"   - æŠ“å–ç¬¬ {page} é ...") # è¨»è§£æ‰é¿å…å¤ªåµ
            
            events, has_data = scrape_single_page(url)
            
            if has_data and events:
                # ğŸ”¥ åœ¨é€™è£¡çµ±ä¸€å¡«å…¥æ—¥æœŸï¼Œçµ•å°ä¸æœƒéŒ¯
                for e in events:
                    e['æ—¥æœŸ'] = clean_date_only
                
                all_data.extend(events)
                page += 1
                time.sleep(0.3)
            else:
                print(f"   âœ… å®Œæˆï¼Œå…± {page-1} é ã€‚")
                break
            
            if page > 30: break # å®‰å…¨ç…è»Š

    # --- è¼¸å‡ºçµæœ ---
    if all_data:
        df = pd.DataFrame(all_data)
        
        # ç¢ºä¿æ¬„ä½é †åº
        cols = ["æ—¥æœŸ", "æ™‚é–“", "æ´»å‹•åç¨±", "åœ°é»", "ä¸»è¬›äºº", "ä¸»æŒäºº", "é¡å‹", "å‚™è¨»", "è©³ç´°å…§å®¹"]
        df = df[cols]
        
        # è¼¸å‡º CSV
        filename = "2026_tibe_events_fixed.csv"
        df.to_csv(filename, index=False, encoding="utf-8-sig")
        
        print("\n" + "="*30)
        print(f"ğŸ‰ æŠ“å–æˆåŠŸï¼")
        print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜: {filename}")
        print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(df)}")
        print("ğŸ’¡ é€™æ¬¡çš„ CSV å·²ç¶“æ¸…é™¤äº†æ›è¡Œç¬¦è™Ÿï¼Œæ‡‰è©²ä¸æœƒå†è·‘ç‰ˆäº†ï¼")
    else:
        print("âŒ æ²’æŠ“åˆ°è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²å€æˆ– ID æ˜¯å¦æ­£ç¢ºã€‚")

if __name__ == "__main__":
    main()